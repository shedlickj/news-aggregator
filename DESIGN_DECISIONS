== Hosting

App is hosted on GitHub under shedlickj/news-aggregator and on Heroku at news-aggregator.heroku.com


== Model
			User
			 |
			 |
	-----------------------------------
	|	 |	   |		  |
      Feed	List	Cluster		Article
					  |
					RssEntry

A User has_many feeds, lists, clusters, and articles, and each belongs_to a User. This decision was made because in all instances, these are parameters that are unique to a user. An Article belongs_to an RssEntry and the reverse relationship is has_many. For each story, there will be one RssEntry, but many articles depending on how many people are following that particular feed. That decision was made to add a layer on top of the rss_entries for specific user state such as favorite and hide.

== ruby-readability & mechanize

The ruby-readability gem was used along with Nokogiri to extract full article text because of the success of the readability javascript code. It was well-tested code with (as advertised) a very high success rate at determining text tags out of html. A few fixes to ruby-readability were necessary but it was a good out-of-the box solution. Additionally, mechanize was used to follow links to get the article html. There was an issue with certain sites redirecting through ad sites, so this solved that issue.

== clustering

From the original links, I used this one called the SpotSigs method: http://glinden.blogspot.com/2008/08/clever-method-of-near-duplicate.html and this article: http://ilpubs.stanford.edu:8090/860/1/2008-10.pdf. In the article, section 3 "Spot Signature Extraction" details the algorithm that I used to determine the spots for each article. Based on their notation, I used aj(1,2), which means I took the 2 words following each antecedent in the list. This is what they used in the example in the article and it gave good evidence that it yielded an accurate portrayal of the document text. Figure 6 shows possible antecedent sets to use, and I chose "a, there, was, said, the, is" because it had a high success rate and only included 6 words vs. 7, which would make it faster. It seemed like a good fit because it was designed for a "large collection of web documents," and I knew that a more heavyweight algorithm might be too slow and would not scale well as the service grows. Also, I could immediately envision how to get it running in Ruby, which was important because I didn't want to spend too much time on the algorithm with only 8 weeks to work. From the original algorithm, I've modified the upper bounds for a match over time to work best with the data set. I also deviated from the article slightly for the bounds of matching and changed the algorithm so that it was more difficult to match articles on the same site. This is because unwanted matches seemed (and still do seem) to be primarily articles from the same site, and I think this is because these pages often use similar styles between articles. Increasing this threshold also made sense because it's unlikely that 2 articles from one site should be a match in the first place because they would be wasting resources to write about the same topic multiple times. The current match parameters are like this:
    - If the articles are from different sources, there are 2 ways to be a match:
        a) there are 4 or more similarities
        b) there are 2 or more similarities and the similarities make up 20% or more of the article (effective for short articles)
    -If the articles are from the same source, there are also 2 ways:
        a) there are 10 or more similarities
        b) there are 4 or more similarities and the similarities make up 50% or more of the article
Finally, I added another case at the end so that an article will attempt to cluster with articles already in a cluster, rather than lone articles even if they are a slightly better match. This is to avoid mini-clusters that are similar, and it works because the lone article eventually will be joined into the larger cluster.

== Article Score

The score of an article is total # of hours since published ( + 5 if in a cluster + 2*size of the cluster). This seems like a good balanced between time since published and an article already being similar to others, and it usually places the first clusters on page 2 of the newsfeed.